import logging
from typing import Any, Dict, List

import pandas as pd
import polars as pl

from heliolib.metadata_extraction_service import MetadataAPI


def _get_tabular_data_from_metadata_api_data(
    data: List[Dict[str, Any]], table_name: str
) -> pd.DataFrame:
    """
    Converts metadata API response data into a tabular Pandas DataFrame format.

    Parameters
    ----------
    data : List[Dict[str, Any]]
        The metadata API response data, expected to be a list of dictionaries.
    table_name : str
        The name of the table for which the data is being processed.

    Returns
    -------
    pd.DataFrame
        A Pandas DataFrame containing the formatted metadata.
    """
    df = pd.json_normalize(data)

    if table_name not in {"attribute", "inverter-model", "panels", "plant"}:
        df.drop(columns=["plant_id"], inplace=True)

    df.drop(columns=["is_deleted"], inplace=True)

    return df


def _get_ag_level_info(data: Dict[str, Any]) -> Dict[str, str]:
    """
    Determines the aggregation level information for a given plant's metadata.

    Parameters
    ----------
    data : Dict[str, Any]
        A dictionary containing plant metadata, including MPPT, string, and combiner existence flags.

    Returns
    -------
    Dict[str, str]
        A dictionary mapping aggregation levels to their respective components (e.g., "ag_level_0": "string").

    Raises
    ------
    ValueError
        If `mppt_exists` is False, indicating that aggregation level info cannot be determined.
    """
    if data["mppt_exists"] and data["analyze_dc"]:
        if data["string_exists"] and data["cb_exists"]:
            ag_level_info = {
                "ag_level_0": "string",
                "ag_level_1": "combiner",
                "ag_level_2": "mppt",
                "ag_level_3": "inv",
            }
        elif data["string_exists"] and not data["cb_exists"]:
            ag_level_info = {
                "ag_level_0": "string",
                "ag_level_1": "mppt",
                "ag_level_2": "inv",
            }
        elif not data["string_exists"] and data["cb_exists"]:
            ag_level_info = {
                "ag_level_0": "combiner",
                "ag_level_1": "mppt",
                "ag_level_2": "inv",
            }
        else:
            ag_level_info = {"ag_level_0": "mppt", "ag_level_1": "inv"}
        return ag_level_info
    else:
        ag_level_info = {"ag_level_0": "inv"}
        return ag_level_info


def _set_ag_level_info_for_array_info(
    ag_level_info: Dict[str, str],
) -> tuple[list[str], list[str]]:
    """
    Generates column names for aggregation levels and their corresponding table ID columns.

    Parameters
    ----------
    ag_level_info : Dict[str, str]
        A dictionary mapping aggregation levels to their respective components.

    Returns
    -------
    tuple[list[str], list[str]]
        A tuple containing:
        - A sorted list of aggregation level column names.
        - A list of corresponding table ID column names.
    """
    ag_level_columns = sorted(ag_level_info.keys(), reverse=True)
    ag_level_attribute_table_id_columns = [
        f"{ag_level_info[col]}_id" for col in ag_level_columns
    ]

    return ag_level_columns, ag_level_attribute_table_id_columns


def _get_system_age_from_reference_date(
    data: Dict[str, Any], reference_date: str
) -> float:
    """
    Calculates the system age in years based on the reference date.

    Parameters
    ----------
    data : Dict[str, Any]
        A dictionary containing plant metadata, including 'system_start_date'.
    reference_date : str
        The reference date in 'YYYY-MM-DD' format.

    Returns
    -------
    float
        The system age in years, rounded to four decimal places.

    Raises
    ------
    ValueError
        If the reference date is before the system start date or has an invalid format.
    """
    try:
        system_start_datetime = pd.to_datetime(
            data["system_start_date"], format="%Y-%m-%d"
        )
        reference_datetime = pd.to_datetime(reference_date, format="%Y-%m-%d")
        system_age = round((reference_datetime - system_start_datetime).days / 365, 4)

        if system_age < 0.0:
            raise ValueError(f"Reference date cannot be before system start date. ")
        return system_age
    except KeyError:
        raise ValueError(
            f'Missing "system_start_date" key in plant data. Please check the metadata.'
        )
    except ValueError:
        raise ValueError(
            f"Invalid (system start date/reference date) format, "
            f'expected date format as "YYYY-MM-DD".'
        )


def _get_plant_attributes_mapping(tracker_exists: bool) -> list[str]:
    """
    Generates a list of plant attribute column names based on tracker existence.

    Parameters
    ----------
    tracker_exists : bool
        A boolean flag indicating whether a tracker exists in the plant metadata.

    Returns
    -------
    list[str]
        A list of column names relevant to the plant attributes.
    """
    if tracker_exists:
        return [
            "plant_id",
            "attributes_id",
            "orientation_id",
            "panel_id",
            "tracker_id",
            "sensor_id",
        ]
    return ["plant_id", "attributes_id", "orientation_id", "panel_id"]


def _validate_and_drop_empty_inputs_from_array_info(data: pd.DataFrame) -> pd.DataFrame:
    """
    Validates the input DataFrame:
    - Raises an error if 'number_of_strings' is zero while 'modules_per_string' is non-zero, or vice versa.
    - Returns the DataFrame after removing rows where both values are zero.

    Parameters
    ----------
    data : pd.DataFrame
        The input DataFrame containing array information.

    Returns
    -------
    pd.DataFrame
        A cleaned DataFrame with rows where both values are zero removed.

    Raises
    ------
    ValueError
        If a row has 'number_of_strings' as zero and 'modules_per_string' as non-zero, or vice versa.
    """
    invalid_rows = data[
        ((data["number_of_strings"] == 0) & (data["modules_per_string"] != 0))
        | ((data["number_of_strings"] != 0) & (data["modules_per_string"] == 0))
    ]

    if not invalid_rows.empty:
        raise ValueError(
            "Error: Found inputs where 'number_of_strings' is zero while 'modules_per_string' is non-zero, "
            "or vice versa."
        )

    return data[
        ~((data["number_of_strings"] == 0) & (data["modules_per_string"] == 0))
    ].copy()


def _attributes_exists(data: Dict[str, Any], attributes_table_name: str) -> bool:
    """
    Checks if attribute data exists for the given plant.

    Parameters
    ----------
    data : Dict[str, Any]
        A dictionary containing plant metadata.
    attributes_table_name : str
        The name of the attributes table to check.

    Returns
    -------
    bool
        True if attribute data exists, False otherwise.

    Raises
    ------
    KeyError
        If the expected key is not found in the data dictionary.
    """
    try:
        return data[f"{attributes_table_name}_exists"]
    except KeyError as e:
        raise KeyError(
            f"Key '{attributes_table_name}_exists' not found in the provided data. "
            f"Ensure the metadata contains the required key."
        ) from e


class SystemInfoMetadataAPI:
    def __init__(self, meta_db_api: MetadataAPI):
        """
        Initialize the SystemInfoMetadataAPI class.

        Parameters
        ----------
        meta_db_api : MetadataAPI
            An instance of MetadataAPI used for data retrieval.
        """

        self.metadb_client = meta_db_api
        self.array_info = None
        self.plant_table = None

    def _validate_plant_id(self, plant_id: str) -> None:
        """
        Validates the plant_id input and ensures it is a non-empty string.

        Parameters
        ----------
        plant_id : str
            The plant ID to be validated.

        Raises
        ------
        TypeError
            If plant_id is not a string.
        ValueError
            If plant_id is an empty string.
        """
        if not isinstance(plant_id, str):
            raise TypeError("plant_id must be a string.")
        if not plant_id.strip():
            raise ValueError("plant_id cannot be an empty string.")

        self.plant_id = plant_id.strip()

    def _validate_reference_date(self, reference_date: str) -> None:
        """
        Validates and sets the reference date.

        Parameters
        ----------
        reference_date : str
            The reference date in 'YYYY-MM-DD' format.

        Raises
        ------
        ValueError
            If the reference date is missing or incorrectly formatted.
        """
        if (
            not reference_date
            or not isinstance(reference_date, str)
            or not reference_date.strip()
        ):
            raise ValueError(
                "Reference Date parameter not provided. Please provide a valid Reference Date in 'YYYY-MM-DD' "
                "format."
            )

        try:
            self.reference_date = pd.to_datetime(
                reference_date, format="%Y-%m-%d"
            ).strftime("%Y-%m-%d")
            logging.info(
                f"Reference Date being used for Meta Data is {self.reference_date}"
            )
        except ValueError:
            raise ValueError(
                "Incorrect Reference Date Format! Please use 'YYYY-MM-DD'."
            )

    def _prepare_plant_data(self) -> Dict[str, Any]:
        """
        Retrieves and prepares plant data, including metadata transformations.

        Returns
        -------
        Dict[str, Any]
            A dictionary containing processed plant data.

        Raises
        ------
        KeyError
            If required keys are missing in the fetched metadata.
        """
        company_data = self.metadb_client.get_company_data_by_plant_id(self.plant_id)
        self.company_id = company_data["company_id"]

        plant_data = self.metadb_client.get_plant_data_by_company_id_and_plant_id(
            self.company_id, self.plant_id
        )

        try:
            plant_data["ID"] = plant_data.pop("plant_id")
            plant_data["plant_name"] = plant_data.pop("plant_id_fk")
            plant_data["altitude"] = plant_data.pop("elevation")
            plant_data["system_age"] = _get_system_age_from_reference_date(
                plant_data, self.reference_date
            )
        except KeyError as e:
            raise KeyError(f"Missing expected key in plant metadata: {e}")

        return plant_data

    def _prepare_inverter_models_data(self, inverters_ids: list[int]) -> pd.DataFrame:
        """
        Fetches and filters inverter model data based on the given inverter IDs.

        Parameters
        ----------
        inverters_ids : list[int]
            A list of inverter IDs to filter the inverter model data.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the filtered inverter model data.
        """
        inverter_models = self.metadb_client.get_all_hardware_models_data_by_table_name(
            table_name="inverter-model"
        )

        inverter_models_df = _get_tabular_data_from_metadata_api_data(
            inverter_models, table_name="inverter-model"
        )

        filtered_inverter_model_df = inverter_models_df[
            inverter_models_df["inverters_id"].isin(inverters_ids)
        ].copy()

        return filtered_inverter_model_df

    def _prepare_panel_models_data(self, panel_ids: list[int]) -> pd.DataFrame:
        """
        Fetches and filters panel model data based on the given panel IDs.

        Parameters
        ----------
        panel_ids : list[int]
            A list of panel IDs to filter the panel model data.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing the filtered panel model data with renamed columns.
        """
        all_panel_models_data = (
            self.metadb_client.get_all_hardware_models_data_by_table_name(
                table_name="panels"
            )
        )

        all_panel_models_data_df = _get_tabular_data_from_metadata_api_data(
            data=all_panel_models_data, table_name="panels"
        )

        filtered_panel_models_df = all_panel_models_data_df[
            all_panel_models_data_df["panel_id"].isin(panel_ids)
        ].copy()

        filtered_panel_models_df.rename(
            columns={"manufacturer": "panel_manufacturer", "wattage": "panel_wattage"},
            inplace=True,
        )

        filtered_panel_models_df[["gamma", "beta", "alpha"]] *= 0.01

        return filtered_panel_models_df

    def _prepare_orientation_data(self) -> pd.DataFrame:
        """
        Retrieves and processes plant orientation data for the given reference month.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed orientation data with validated tilt and azimuth values.

        Raises
        ------
        ValueError
            If values in 'surface_tilt' are not in the range [0, 90].
            If values in 'surface_azimuth' are not in the range [0, 359].
        """
        orientation_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="orientation"
        )

        orientation_data_df = _get_tabular_data_from_metadata_api_data(
            data=orientation_data, table_name="orientation"
        )

        date_month = pd.to_datetime(self.reference_date).strftime("%b").lower()
        selected_month_orientation_data_df = orientation_data_df[
            ["orientation_id", f"{date_month}_data"]
        ].copy()

        selected_month_orientation_data_df[["surface_tilt", "surface_azimuth"]] = (
            selected_month_orientation_data_df[f"{date_month}_data"]
            .str.split(",", expand=True)
            .astype(float)
        )

        if not selected_month_orientation_data_df["surface_tilt"].between(0, 90).all():
            raise ValueError(
                "Values in 'surface_tilt' column are not in the range [0, 90]."
            )

        if (
            not selected_month_orientation_data_df["surface_azimuth"]
            .between(0, 359)
            .all()
        ):
            raise ValueError(
                "Values in 'surface_azimuth' column are not in the range [0, 359]."
            )

        selected_month_orientation_data_df.drop(
            columns=[f"{date_month}_data"], inplace=True
        )

        return selected_month_orientation_data_df

    def _prepare_trackers(self) -> pd.DataFrame:
        """
        Retrieves and processes tracker data for the given plant.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed tracker data.
        """
        tracker_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            plant_id=self.plant_id, company_id=self.company_id, table_name="tracker"
        )

        tracker_data_df = _get_tabular_data_from_metadata_api_data(
            data=tracker_data, table_name="tracker"
        )

        return tracker_data_df

    def _prepare_inverters(self) -> pd.DataFrame:
        """
        Retrieves and processes inverter data, merging it with the corresponding inverter models.
        Only keeps inverters that have a matching model.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed inverter data with model details.
        """
        inverter_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="inverter"
        )

        inverter_data_df = _get_tabular_data_from_metadata_api_data(
            data=inverter_data, table_name="inverter"
        )

        inverter_models_data = self._prepare_inverter_models_data(
            inverters_ids=inverter_data_df["inverters_id"].unique().tolist()
        )

        inverters_table_data_df = inverter_data_df.merge(
            inverter_models_data, on="inverters_id", how="inner"
        )

        return inverters_table_data_df

    def _prepare_mppts(self, ag_level_info: Dict[str, str]) -> pd.DataFrame:
        """
        Retrieves and processes MPPT data for the given plant, modifying columns based on aggregation level info.

        Parameters
        ----------
        ag_level_info : Dict[str, str]
            A dictionary specifying the aggregation levels of the system.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed MPPT data.
        """
        mppt_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="mppt"
        )

        mppt_data_df = _get_tabular_data_from_metadata_api_data(
            data=mppt_data, table_name="mppt"
        )

        if (ag_level_info["ag_level_0"] != "mppt") and (
            "mppt" in ag_level_info.values()
        ):
            mppt_data_df.drop(
                columns=["number_of_strings", "modules_per_string", "inv_id"],
                inplace=True,
            )
        else:
            mppt_data_df.drop(columns=["inv_id"], inplace=True)

        return mppt_data_df

    def _prepare_combiners(self, ag_level_info: Dict[str, str]) -> pd.DataFrame:
        """
        Retrieves and processes combiner data for the given plant, modifying columns based on aggregation level info.

        Parameters
        ----------
        ag_level_info : Dict[str, str]
            A dictionary specifying the aggregation levels of the system.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed combiner data.
        """
        combiner_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="combiner"
        )

        combiner_data_df = _get_tabular_data_from_metadata_api_data(
            data=combiner_data, table_name="combiner"
        )

        if (ag_level_info["ag_level_0"] != "combiner") and (
            "combiner" in ag_level_info.values()
        ):
            combiner_data_df.drop(
                columns=[
                    "number_of_strings",
                    "modules_per_string",
                    "mppt_id",
                    "inv_id",
                ],
                inplace=True,
            )
        else:
            combiner_data_df.drop(columns=["mppt_id", "inv_id"], inplace=True)

        return combiner_data_df

    def _prepare_strings(self, ag_level_info: Dict[str, str]) -> pd.DataFrame:
        """
        Retrieves and processes string data for the given plant, modifying columns based on aggregation level info.

        Parameters
        ----------
        ag_level_info : Dict[str, str]
            A dictionary specifying the aggregation levels of the system.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed string data.

        Raises
        ------
        ValueError
            If aggregation level info is incorrect or if there is no data in the string table for the given plant.
        """
        string_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="string"
        )

        string_data_df = _get_tabular_data_from_metadata_api_data(
            data=string_data, table_name="string"
        )

        if (ag_level_info["ag_level_0"] == "string") and (not string_data_df.empty):
            # drop rows where "modules_per_string" is 0
            string_data_df = string_data_df[
                string_data_df["modules_per_string"] != 0
            ].copy()
            string_data_df["number_of_strings"] = 1
            string_data_df.drop(
                columns=["combiner_id", "mppt_id", "inv_id"], inplace=True
            )
        else:
            raise ValueError(
                f"Incorrect ag_level_info mapping or No data in String table for plant_id: {self.plant_id}"
            )

        return string_data_df

    def _prepare_attributes(self) -> pd.DataFrame:
        """
        Retrieves and processes attribute data for the given plant.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing processed attribute data.
        """
        attributes_data = self.metadb_client.get_plant_metadata_table_by_plant_id(
            company_id=self.company_id, plant_id=self.plant_id, table_name="attribute"
        )

        attributes_data_df = _get_tabular_data_from_metadata_api_data(
            data=attributes_data, table_name="attribute"
        )

        return attributes_data_df

    def _prepare_array_info(self, data: Dict[str, Any]) -> pd.DataFrame:
        """
        Constructs and processes array information by merging various plant metadata components.

        Parameters
        ----------
        data : Dict[str, Any]
            A dictionary containing plant metadata, including tracking system and component existence flags.

        Returns
        -------
        pd.DataFrame
            A DataFrame containing structured array information with relevant metadata.
        """
        array_info = pd.DataFrame()
        ag_level_info = _get_ag_level_info(data=data)

        ag_level_columns, ag_level_attribute_table_id_columns = (
            _set_ag_level_info_for_array_info(ag_level_info=ag_level_info)
        )
        mppt_exists = _attributes_exists(data, "mppt")
        analyze_dc = data["analyze_dc"]
        # mppt_exists will now on depends upon analyze_dc, we will not send mppts to service if analyze_dc is false
        mppt_exists = analyze_dc
        tracker_exists = _attributes_exists(data, "tracker")
        combiner_exists = _attributes_exists(data, "cb")
        string_exists = _attributes_exists(data, "string")
        inverter_data = self._prepare_inverters()
        if mppt_exists:
            attributes_data_columns = _get_plant_attributes_mapping(tracker_exists)
            attributes_data = self._prepare_attributes()

            array_info[ag_level_columns] = attributes_data[
                ag_level_attribute_table_id_columns
            ].copy()
            array_info[ag_level_attribute_table_id_columns] = attributes_data[
                ag_level_attribute_table_id_columns
            ].copy()
            array_info[attributes_data_columns] = attributes_data[
                attributes_data_columns
            ].copy()

            array_info = array_info.merge(inverter_data, on="inv_id")
            mppt_data = self._prepare_mppts(ag_level_info=ag_level_info)
            array_info = array_info.merge(mppt_data, on="mppt_id")

            if combiner_exists:
                combiner_data = self._prepare_combiners(ag_level_info=ag_level_info)
                array_info = array_info.merge(combiner_data, on="combiner_id")

            if string_exists:
                string_data = self._prepare_strings(ag_level_info=ag_level_info)
                array_info = array_info.merge(string_data, on="string_id")

            array_info = _validate_and_drop_empty_inputs_from_array_info(
                data=array_info
            )
            array_info["number_of_modules"] = (
                array_info["modules_per_string"] * array_info["number_of_strings"]
            )

            orientation_data = self._prepare_orientation_data()
            array_info = array_info.merge(orientation_data, on="orientation_id").drop(
                columns=["orientation_id"]
            )

            panel_data = self._prepare_panel_models_data(
                array_info["panel_id"].unique().tolist()
            )
            array_info = array_info.merge(panel_data, on="panel_id")

            array_info["installed_capacity"] = (
                array_info["number_of_modules"] * array_info["panel_wattage"]
            )

            if data["system_age"] > 1:
                array_info["expected_degradation"] = array_info["degrdn_yr1"] + (
                    (data["system_age"] - 1) * array_info["degrdn_yr2"]
                )
            else:
                array_info["expected_degradation"] = array_info["degrdn_yr1"]
        else:
            array_info[ag_level_columns] = inverter_data[
                ag_level_attribute_table_id_columns
            ].copy()
            array_info[ag_level_attribute_table_id_columns] = inverter_data[
                ag_level_attribute_table_id_columns
            ].copy()
            array_info = array_info.merge(inverter_data, on="inv_id")
        if tracker_exists:
            tracker_data = self._prepare_trackers()
            array_info = array_info.merge(tracker_data, on="tracker_id")
        array_info.set_index(ag_level_columns, inplace=True)
        array_info.sort_index(inplace=True)

        return array_info

    def get_plant_meta_data(
        self, plant_id: str, reference_date: str
    ) -> tuple[pd.DataFrame, pd.DataFrame]:
        """
        Retrieves and processes plant metadata, returning structured plant and array info tables.

        Parameters
        ----------
        plant_id : str
            The unique identifier for the plant.
        reference_date : str
            The reference date in 'YYYY-MM-DD' format.

        Returns
        -------
        tuple[pd.DataFrame, pd.DataFrame]
            - A DataFrame containing structured array information.
            - A DataFrame containing structured plant metadata.
        """
        self._validate_plant_id(plant_id)
        self._validate_reference_date(reference_date)

        plant_data = self._prepare_plant_data()

        self.array_info = self._prepare_array_info(data=plant_data)
        self.plant_table = _get_tabular_data_from_metadata_api_data(
            data=[plant_data], table_name="plant"
        )

        return self.plant_table, self.array_info

    def get_plant_meta_data_as_polars_dataframes(
        self, plant_id: str, reference_date: str
    ) -> tuple[pl.DataFrame, pl.DataFrame]:
        """
        Converts the plant metadata and array info tables to Polars DataFrames.

        Parameters:
        -----------
        plant_id : str
            The unique identifier for the plant.
        reference_date : str
            The reference date in 'YYYY-MM-DD' format.

        Returns:
        -------
        tuple[pl.DataFrame, pl.DataFrame]
            - A Polars DataFrame containing structured array information.
            - A Polars DataFrame containing structured plant metadata.
        """
        # call the above function to get the pandas dataframes
        plant_table, array_info = self.get_plant_meta_data(
            plant_id=plant_id, reference_date=reference_date
        )

        # convert the pandas dataframes to polars dataframes
        plant_table = pl.from_pandas(plant_table)
        array_info = pl.from_pandas(array_info)

        return plant_table, array_info
